version: '3.8'

services:
  ollama:
    container_name: ollama
    image: ollama/ollama:latest
    ports:
      - "11434:11434"
    volumes:
      - ollama_data:/root/.ollama
    environment:
      - OLLAMA_ORIGINS=http://localhost:3000,http://localhost:8000
      - OLLAMA_HOST=0.0.0.0
    healthcheck:
      test: ["CMD", "ollama", "-v"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 20s
    restart: unless-stopped

  open-webui:
    container_name: open-webui
    image: ghcr.io/open-webui/open-webui:v0.6.15
    ports:
      - "3000:8080"
    environment:
      - CORS_ALLOW_ORIGIN=*
      - FORWARDED_ALLOW_IPS=*
      - SCARF_NO_ANALYTICS=true
      - DO_NOT_TRACK=true
      - ANONYMIZED_TELEMETRY=false
      - OLLAMA_BASE_URL=http://ollama:11434
      - OPENAI_API_BASE_URL=http://rag-api:8000/v1
      - OPENAI_API_KEY=dummy-key
      - AUTOMATIC1111_BASE_URL=http://localhost:7860
      - DEFAULT_MODEL=llama4
      - WEBUI_JWT_SECRET_KEY=your-secret-key
      - WEBUI_SECRET_KEY=your-secret-key
      - API_HOST=0.0.0.0
      - APP_HOST=0.0.0.0
      - APP_PORT=8080
      - MODEL_CONTEXT_SIZE=4096
    volumes:
      - webui_data:/app/backend/data
    depends_on:
      ollama:
        condition: service_healthy
    restart: unless-stopped

  rag-api:
    build: .
    ports:
      - "8000:8000"
    volumes:
      - ./documents:/app/documents
      - ./.vector_store:/app/vector_store
    environment:
      - OLLAMA_BASE_URL=http://ollama:11434
      - MODEL_NAME=llama4
      - CONTEXT_SIZE=4096
    depends_on:
      ollama:
        condition: service_healthy
    restart: unless-stopped

volumes:
  ollama_data:
  webui_data:
